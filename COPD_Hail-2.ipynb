{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo COPDGene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Hail and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time import hail as hl\n",
    "import hail.expr.aggregators as agg\n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using Hail, we import some standard Python libraries for use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log, isnan\n",
    "from pprint import pprint\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Span\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the mt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import_vcf() takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order (e.g., a dataset split by chromosome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vds = hl.import_vcf(\"list_of_the_vcf_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vds.write('gs://versmee/phs000951.phg000892.v1.TOPMed_WGS_COPDGene_v2.genotype-calls-vcf.WGS_markerset_grc37/vds/phs000951.phg000892.v1.TOPMed_WGS_COPDGene_v2.genotype-calls-vcf.WGS_markerset_grc37.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and annotating the matrix table\n",
    "### Loading data from the cloud storage\n",
    "Hail has its own internal data representation, called a MatrixTable. This is both an on-disk file format and a Python object. Here, we read a MatrixTable from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = hl.read_matrix_table(\"gs://versmee/phs000951.phg000892.v1.TOPMed_WGS_COPDGene_v2.genotype-calls-vcf.WGS_markerset_grc37/vds/phs000951.phg000892.v1.TOPMed_WGS_COPDGene_v2.genotype-calls-vcf.WGS_markerset_grc37.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the speed of the analysis, let's filter our matrix table to keep only the first 250 samples, and the chromosome 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_keep = {'NWD387864',\n",
    "'NWD981858',\n",
    "'NWD345740',\n",
    "'NWD106974',\n",
    "'NWD495242',\n",
    "'NWD238571',\n",
    "'NWD516168',\n",
    "'NWD454326',\n",
    "'NWD355594',\n",
    "'NWD313803',\n",
    "'NWD987980',\n",
    "'NWD648146',\n",
    "'NWD480965',\n",
    "'NWD476196',\n",
    "'NWD776146',\n",
    "'NWD424521',\n",
    "'NWD169136',\n",
    "'NWD926215',\n",
    "'NWD443374',\n",
    "'NWD298631',\n",
    "'NWD174669',\n",
    "'NWD768113',\n",
    "'NWD443706',\n",
    "'NWD199320',\n",
    "'NWD262428',\n",
    "'NWD277157',\n",
    "'NWD884540',\n",
    "'NWD106346',\n",
    "'NWD854755',\n",
    "'NWD805136',\n",
    "'NWD103241',\n",
    "'NWD520174',\n",
    "'NWD622621',\n",
    "'NWD863691',\n",
    "'NWD347934',\n",
    "'NWD794539',\n",
    "'NWD303390',\n",
    "'NWD972387',\n",
    "'NWD513493',\n",
    "'NWD749293',\n",
    "'NWD548714',\n",
    "'NWD432175',\n",
    "'NWD992598',\n",
    "'NWD321762',\n",
    "'NWD173999',\n",
    "'NWD521195',\n",
    "'NWD375222',\n",
    "'NWD588609',\n",
    "'NWD470074',\n",
    "'NWD170342',\n",
    "'NWD608797',\n",
    "'NWD990794',\n",
    "'NWD403833',\n",
    "'NWD552162',\n",
    "'NWD934239',\n",
    "'NWD507705',\n",
    "'NWD302840',\n",
    "'NWD357442',\n",
    "'NWD509184',\n",
    "'NWD726547',\n",
    "'NWD314204',\n",
    "'NWD561228',\n",
    "'NWD837565',\n",
    "'NWD297040',\n",
    "'NWD701651',\n",
    "'NWD242023',\n",
    "'NWD848651',\n",
    "'NWD472709',\n",
    "'NWD470467',\n",
    "'NWD595890',\n",
    "'NWD535377',\n",
    "'NWD958422',\n",
    "'NWD963149',\n",
    "'NWD897564',\n",
    "'NWD873976',\n",
    "'NWD518498',\n",
    "'NWD111182',\n",
    "'NWD188499',\n",
    "'NWD601291',\n",
    "'NWD962140',\n",
    "'NWD659875',\n",
    "'NWD638623',\n",
    "'NWD254235',\n",
    "'NWD692778',\n",
    "'NWD318913',\n",
    "'NWD965920',\n",
    "'NWD394153',\n",
    "'NWD690263',\n",
    "'NWD892599',\n",
    "'NWD772712',\n",
    "'NWD717106',\n",
    "'NWD666236',\n",
    "'NWD737986',\n",
    "'NWD335297',\n",
    "'NWD931190',\n",
    "'NWD583212',\n",
    "'NWD527579',\n",
    "'NWD880952',\n",
    "'NWD662898',\n",
    "'NWD188576',\n",
    "'NWD552381',\n",
    "'NWD313341',\n",
    "'NWD307204',\n",
    "'NWD532206',\n",
    "'NWD524562',\n",
    "'NWD930314',\n",
    "'NWD201389',\n",
    "'NWD613652',\n",
    "'NWD145425',\n",
    "'NWD942869',\n",
    "'NWD880283',\n",
    "'NWD775481',\n",
    "'NWD838276',\n",
    "'NWD836583',\n",
    "'NWD431356',\n",
    "'NWD939409',\n",
    "'NWD610349',\n",
    "'NWD840747',\n",
    "'NWD499382',\n",
    "'NWD820210',\n",
    "'NWD720320',\n",
    "'NWD964757',\n",
    "'NWD124202',\n",
    "'NWD549073',\n",
    "'NWD685210',\n",
    "'NWD144752',\n",
    "'NWD690395',\n",
    "'NWD546198',\n",
    "'NWD266700',\n",
    "'NWD166122',\n",
    "'NWD583862',\n",
    "'NWD726953',\n",
    "'NWD464021',\n",
    "'NWD341202',\n",
    "'NWD732461',\n",
    "'NWD545684',\n",
    "'NWD778353',\n",
    "'NWD492145',\n",
    "'NWD290608',\n",
    "'NWD744054',\n",
    "'NWD445762',\n",
    "'NWD914370',\n",
    "'NWD375516',\n",
    "'NWD623117',\n",
    "'NWD286599',\n",
    "'NWD836978',\n",
    "'NWD991940',\n",
    "'NWD168631',\n",
    "'NWD250657',\n",
    "'NWD660054',\n",
    "'NWD865894',\n",
    "'NWD251286',\n",
    "'NWD716015',\n",
    "'NWD140578',\n",
    "'NWD312078',\n",
    "'NWD754588',\n",
    "'NWD483144',\n",
    "'NWD247496',\n",
    "'NWD477088',\n",
    "'NWD379919',\n",
    "'NWD671709',\n",
    "'NWD988599',\n",
    "'NWD211358',\n",
    "'NWD909344',\n",
    "'NWD840543',\n",
    "'NWD323996',\n",
    "'NWD137774',\n",
    "'NWD675038',\n",
    "'NWD522896',\n",
    "'NWD639213',\n",
    "'NWD201086',\n",
    "'NWD392402',\n",
    "'NWD407190',\n",
    "'NWD175512',\n",
    "'NWD343061',\n",
    "'NWD782082',\n",
    "'NWD923028',\n",
    "'NWD549858',\n",
    "'NWD510650',\n",
    "'NWD856888',\n",
    "'NWD134022',\n",
    "'NWD992018',\n",
    "'NWD552521',\n",
    "'NWD451099',\n",
    "'NWD631188',\n",
    "'NWD617513',\n",
    "'NWD429834',\n",
    "'NWD426061',\n",
    "'NWD941708',\n",
    "'NWD821619',\n",
    "'NWD949330',\n",
    "'NWD760555',\n",
    "'NWD635435',\n",
    "'NWD802228',\n",
    "'NWD276977',\n",
    "'NWD677010',\n",
    "'NWD322197',\n",
    "'NWD528686',\n",
    "'NWD421666',\n",
    "'NWD685409',\n",
    "'NWD931951',\n",
    "'NWD805308',\n",
    "'NWD399371',\n",
    "'NWD996433',\n",
    "'NWD336689',\n",
    "'NWD471830',\n",
    "'NWD244248',\n",
    "'NWD432945',\n",
    "'NWD935964',\n",
    "'NWD848157',\n",
    "'NWD398277',\n",
    "'NWD551753',\n",
    "'NWD174224',\n",
    "'NWD514522',\n",
    "'NWD157837',\n",
    "'NWD957332',\n",
    "'NWD140315',\n",
    "'NWD623755',\n",
    "'NWD339234',\n",
    "'NWD347426',\n",
    "'NWD408353',\n",
    "'NWD204810',\n",
    "'NWD646771',\n",
    "'NWD588119',\n",
    "'NWD813787',\n",
    "'NWD433887',\n",
    "'NWD373981',\n",
    "'NWD176857',\n",
    "'NWD536777',\n",
    "'NWD261689',\n",
    "'NWD368721',\n",
    "'NWD240366',\n",
    "'NWD600857',\n",
    "'NWD351418',\n",
    "'NWD530831',\n",
    "'NWD636594',\n",
    "'NWD676552',\n",
    "'NWD768682',\n",
    "'NWD856054',\n",
    "'NWD315080',\n",
    "'NWD700572',\n",
    "'NWD419358',\n",
    "'NWD259487',\n",
    "'NWD513972',\n",
    "'NWD444356',\n",
    "'NWD840009',\n",
    "'NWD140173',\n",
    "'NWD217713',\n",
    "'NWD315835',\n",
    "'NWD782889',\n",
    "'NWD882772',\n",
    "'NWD491062',\n",
    "'NWD317869'}\n",
    "set_to_keep = hl.literal(samples_to_keep)\n",
    "dataset = vds.filter_cols(set_to_keep.contains(vds['s']))\n",
    "intervals = [hl.parse_locus_interval(x) for x in ['22']]\n",
    "ds = hl.filter_intervals(dataset, intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know our data\n",
    "It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of these methods are demonstrated below.\n",
    "\n",
    "The `rows` method can be used to get a table with all the row fields in our MatrixTable.\n",
    "\n",
    "We can use `rows` along with `select` to pull out 5 variants. The `select` method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.\n",
    "\n",
    "Use the `show` method to display the variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vds.rows().select().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to peek at the first few sample IDs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vds.s.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the first few genotype calls, we can use `entries` along with `select` and `take`. The `take` method collects the first n rows into a list. Alternatively, we can use the `show` method, which prints the first n rows to the console in a table format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vds.entry.take(5)\n",
    "vds.entry.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column fields\n",
    "A Hail MatrixTable can have any number of row fields and column fields for storing data associated with each row and column. Annotations are usually a critical part of any genetic study. Column fields are where you’ll store information about sample phenotypes, ancestry, sex, and covariates. Row fields can be used to store information like gene membership and functional impact for use in QC or analysis.\n",
    "\n",
    "We import a delimited text file (text table), with phenotypics characteristics, as `Table` to annotate our genomics matrix table.\n",
    "This file can be imported into Hail with import_table. This method produces a Table object. Think of this as a Pandas or R dataframe that isn’t limited by the memory on your machine – behind the scenes, it’s distributed with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = (hl.import_table('gs://versmee/COPDannotations.txt', \n",
    "                         impute=True,\n",
    "                         types={'SaO2':hl.tfloat, 'Heart_Rate':hl.tfloat, 'is_female':hl.tfloat, 'AffectionBool':hl.tfloat, 'SmokerBool':hl.tfloat}, \n",
    "                         missing='')\n",
    "         .key_by('Samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to peek at the structure of a `Table` is to look at its `schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To peek at the first few values, use the `show` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table.show(width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `annotate_cols` method to join the table with the MatrixTable containing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = ds.annotate_cols(**table[ds.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query functions and the Hail Expression Language\n",
    "Hail has a number of useful query functions that can be used for gathering statistics on our dataset. These query functions take Hail Expressions as arguments.\n",
    "\n",
    "We will start by looking at some statistics of the information in our table. The `aggregate` method can be used to aggregate over rows of the table.\n",
    "\n",
    "`counter` is an aggregation function that counts the number of occurrences of each unique element. We can use this to pull out the population distribution by passing in a Hail Expression for the field that we want to count by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(table.aggregate(agg.counter(table.Affection)))\n",
    "pprint(table.aggregate(agg.counter(table.Race)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stats` is an aggregation function that produces some useful statistics about numeric collections. We can use this to see the distribution of the Heart_Rate phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(table.aggregate(agg.stats(table.Heart_Rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality demonstrated in the last few cells isn’t anything especially new: it’s certainly not difficult to ask these questions with Pandas or R dataframes, or even Unix tools like `awk`. But Hail can use the same interfaces and query language to analyze collections that are much larger, like the set of variants.\n",
    "\n",
    "Here we calculate the counts of each of the 12 possible unique SNPs (4 choices for the reference base * 3 choices for the alternate base).\n",
    "\n",
    "To do this, we need to get the alternate allele of each variant and then count the occurences of each unique ref/alt pair. This can be done with Hail’s `counter` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_counts = vds.aggregate_rows(agg.counter(hl.Struct(ref=vds.alleles[0], alt=vds.alleles[1])))\n",
    "pprint(snp_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the counts in descending order using Python’s Counter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(snp_counts)\n",
    "counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control\n",
    "QC is entirely based on the ability to understand the properties of a dataset. Hail attempts to make this easier by providing the sample_qc method, which produces a set of useful metrics and stores them in a column field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds.col.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = hl.sample_qc(vds)\n",
    "vds.col.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interoperability is a big part of Hail.\n",
    "\n",
    "To pull out these new metrics, we need to convert to a Pandas DataFrame, which can be done using the the `to_pandas` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vds.cols().to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the QC metrics is a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(data, bins=50):\n",
    "    freqs, edges = np.histogram(data, density=True, bins=bins)\n",
    "    return {'bottom': 0, 'top': freqs, 'left': edges[:-1], 'right': edges[1:], 'line_color': 'black'}\n",
    "p = figure(x_axis_label='Call Rate', y_axis_label='Frequency')\n",
    "p.quad(**hist(df['sample_qc.call_rate']))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = vds.filter_cols((vds.sample_qc.call_rate >= 0.97))\n",
    "print('After filter, %d/1886 samples remain.' % vds.count_cols())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant QC\n",
    "We can use the `variant_qc` method to produce a variety of useful statistics, plot them, and filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds.row.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cache` is used to optimize some of the downstream operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = hl.variant_qc(vds).cache()\n",
    "vds.row.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s do a GWAS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (case vs controls) in the dataset, we are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas = hl.logistic_regression(\n",
    "            test='wald',\n",
    "            y=vds.AffectionBool,\n",
    "            x=vds.GT.n_alt_alleles(),\n",
    "            covariates=[vds.Age])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method performs, for each row, a significance test of the input variable in predicting a binary (case-control) response variable based on the logistic regression model. The response variable type must either be numeric (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.\n",
    "\n",
    "Hail supports the Wald test `wald`, likelihood ratio test `lrt`, Rao score test `score`, and Firth test `firth`. Hail only includes columns for which the response variable and all covariates are defined. For each row, Hail imputes missing input values as the mean of the non-missing values.\n",
    "\n",
    "The example above considers a model of the form\n",
    "##### Prob(AffectionBool)=sigmoid(β0+β1gt+β2age+β3is.female+ε),ε∼N(0,σ2)\n",
    "where sigmoid is the sigmoid function, the genotype gtis coded as 0 for HomRef, 1 for Het, and 2 for HomVar, and the covariate is.female is coded as for 1 for True (female) and 0 for False (male). The null model sets β1=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas.row.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.\n",
    "\n",
    "Python makes it easy to make a Q-Q (quantile-quantile) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qqplot(pvals):\n",
    "    spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals))\n",
    "    exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]\n",
    "    obs = [-log(p, 10) for p in spvals]\n",
    "    p = figure(title='Q-Q Plot',\n",
    "               x_axis_label='Expected p-value (-log10 scale)',\n",
    "               y_axis_label='Observed p-value (-log10 scale)')\n",
    "    p.scatter(x=exp, y=obs, color='black')\n",
    "    bound = max(max(exp), max(obs)) * 1.1\n",
    "    p.line([0, bound], [0, bound], color='red')\n",
    "    show(p)\n",
    "qqplot(gwas.logreg.p_value.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking confounded factors into account\n",
    "\n",
    "The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to COPD (unlikely), or there’s a confounder.\n",
    "\n",
    "The pca method produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca method does the same, using HWE-normalized genotypes for the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(vds.GT, k = 5)\n",
    "pprint(pca_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scores.show(5, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve got principal components per sample, we may as well plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_table = (\n",
    "    pca_scores.select(Race=vds.cols()[pca_scores.s].Race,\n",
    "                      PC1=pca_scores.scores[0],\n",
    "                      PC2=pca_scores.scores[1])).to_pandas()\n",
    "\n",
    "p = figure(title='PCA', background_fill_color='#EEEEEE',\n",
    "          x_axis_label='PC1', y_axis_label='PC2')\n",
    "pop_colors = {'African American': 'green', ' Caucasian': 'red'}\n",
    "\n",
    "for pop, color in pop_colors.items():\n",
    "    samples = pca_table[pca_table['Race'] == pop]\n",
    "    p.scatter(samples[\"PC1\"], samples[\"PC2\"],\n",
    "              color=color,\n",
    "             alpha=0.5,\n",
    "             legend=pop,\n",
    "              size=8)\n",
    "\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rerun our logistic regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vds = vds.annotate_cols(pca = pca_scores[vds.s])\n",
    "\n",
    "gwas2 = hl.logistic_regression(\n",
    "            test='wald',\n",
    "            y=vds.AffectionBool,\n",
    "            x=vds.GT.n_alt_alleles(),\n",
    "            covariates=[vds.Age, vds.pca.scores[0]])\n",
    "\n",
    "pvals = gwas2.logreg.p_value.collect()\n",
    "qqplot(pvals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}